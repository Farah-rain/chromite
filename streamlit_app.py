
import streamlit as st
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import os
import joblib
import requests
import base64
from io import BytesIO

# -------------------- È°µÈù¢ÈÖçÁΩÆÔºà‰ªÖ UI Ëã±ÊñáÔºâ --------------------
st.set_page_config(page_title="Chromite Extraterrestrial Origin Classifier", layout="wide")
st.title("‚ú® Chromite Extraterrestrial Origin Classifier")

# -------------------- Â∏∏Èáè‰∏éÊò†Â∞ÑÔºà‰∏éËÆ≠ÁªÉ‰∏ÄËá¥ÔºõÊ≥®Èáä‰∏≠ÊñáÔºâ --------------------
ABSTAIN_LABEL = "Unknown"  # Unknown Ê†áÁ≠æÁªü‰∏ÄÂè£ÂæÑ
THRESHOLDS = {"Level2": 0.90, "Level3": 0.90}  # Level2/Level3 ÁöÑÊîæË°åÈòàÂÄº
# Áà∂Á±ªÈîÆ‰∏éÂ≠êÁ±ªÈõÜÂêàÔºàÊ≥®ÊÑèÔºöËã•Ê®°ÂûãÊ≤°ÊúâËÅöÂêàÁ±ª "UOC"ÔºåÈúÄÊîπÊàê UOC-H/L/LLÔºâ
valid_lvl3 = {
    "OC": {"EOC-H", "EOC-L", "EOC-LL", "UOC"},
    "CC": {"CM-CO", "CR-clan", "CV"}
}

# -------------------- Â∞èÂ∑•ÂÖ∑ÂáΩÊï∞ÔºàÊ≥®Èáä‰∏≠ÊñáÔºâ --------------------
def apply_threshold(proba: np.ndarray, classes: np.ndarray, thr: float):
    """ÂØπÂàÜÁ±ªÊ¶ÇÁéáÂ∫îÁî®ÈòàÂÄºÔºöÊúÄÂ§ßÊ¶ÇÁéá>=thr Êó∂ËæìÂá∫ËØ•Á±ªÂà´ÔºåÂê¶Âàô Unknown„ÄÇËøîÂõû(È¢ÑÊµã, ÊúÄÂ§ßÊ¶ÇÁéá)„ÄÇ"""
    max_idx = np.argmax(proba, axis=1)
    max_val = proba[np.arange(proba.shape[0]), max_idx]
    pred = np.where(max_val >= thr, classes[max_idx], ABSTAIN_LABEL)
    return pred, max_val

@st.cache_resource
def load_model_and_metadata():
    """Ê®°Âûã‰∏éÁâπÂæÅÂàóÂä†ËΩΩÔºö‰ºòÂÖà models/ ÁõÆÂΩïÔºõÁâπÂæÅÂàó‰ºòÂÖà JSON ÈÄÄÂõû model.feature_name_„ÄÇ"""
    def _load(path1, path2):
        return joblib.load(path1) if os.path.exists(path1) else joblib.load(path2)

    model_lvl1 = _load("models/model_level1.pkl", "model_level1.pkl")
    model_lvl2 = _load("models/model_level2.pkl", "model_level2.pkl")
    model_lvl3 = _load("models/model_level3.pkl", "model_level3.pkl")

    feat_json = "models/feature_columns.json" if os.path.exists("models/feature_columns.json") else "feature_columns.json"
    if os.path.exists(feat_json):
        import json
        with open(feat_json, "r", encoding="utf-8") as f:
            features = json.load(f)
    else:
        features = getattr(model_lvl1, "feature_name_", None)
        if not features:
            st.error("Feature columns not found (feature_columns.json or model.feature_name_).")
            st.stop()
    return model_lvl1, model_lvl2, model_lvl3, features

# ==== Êñ∞ÁöÑ explainer ÁºìÂ≠òÂÆûÁé∞ÔºöÁî®‚ÄúÁ≠æÂêç + ‰∏ãÂàíÁ∫øÂèÇÊï∞‚ÄùÈÅøÂÖçÂØπÊ®°ÂûãÂØπË±°ÂÅöÂìàÂ∏å ====
@st.cache_resource
def _make_explainer_cached(sig: str, _model):
    """ÁºìÂ≠ò SHAP explainerÔºõsig ‰Ωú‰∏∫ÁºìÂ≠òÈîÆÔºå_model ‰∏çÂèÇ‰∏éÂìàÂ∏å„ÄÇ"""
    return shap.TreeExplainer(_model)

def _model_signature(model) -> str:
    """ÊûÑÈÄ†‰∏Ä‰∏™ÂèØÂìàÂ∏åÁöÑÊ®°ÂûãÁ≠æÂêçÔºöÊ®°ÂûãÁ±ªÂêç + ÊéíÂ∫èÂêéÁöÑË∂ÖÂèÇ + Á±ªÂà´ÂàóË°®"""
    try:
        params = model.get_params()
        params_tup = tuple(sorted((k, str(v)) for k, v in params.items()))
    except Exception:
        params_tup = ()
    try:
        classes = tuple(map(str, getattr(model, "classes_", ())))
    except Exception:
        classes = ()
    return f"{model.__class__.__name__}|{hash(params_tup)}|{hash(classes)}"

def preprocess_uploaded_data(df):
    """
    Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÔºöÂÖºÂÆπ FeOT Áº∫Â§±ÁöÑÊãÜÂàÜÔºåÊ¥æÁîüÁâπÂæÅÁîüÊàêÔºà‰∏é‰Ω†ÂéüÈÄªËæë‰∏ÄËá¥Ôºâ„ÄÇ
    """
    MW = {'TiO2':79.866,'Al2O3':101.961,'Cr2O3':151.99,'FeO':71.844,'MnO':70.937,'MgO':40.304,'ZnO':81.38,'SiO2':60.0843,'V2O3':149.88}
    O_num = {'TiO2':2,'Al2O3':3,'Cr2O3':3,'FeO':1,'MnO':1,'MgO':1,'ZnO':1,'SiO2':2,'V2O3':3}
    Cat_num={'TiO2':1,'Al2O3':2,'Cr2O3':2,'FeO':1,'MnO':1,'MgO':1,'ZnO':1,'SiO2':1,'V2O3':2}
    FE2O3_OVER_FEO_FE_EQ = 159.688 / (2 * 71.844)

    for ox in MW:
        if ox not in df.columns:
            df[ox] = 0.0

    df = df.copy()

    use_manual_fe = "FeO" in df.columns and "Fe2O3" in df.columns
    if use_manual_fe:
        # Ëã•Â∑≤ÁªôÂá∫ FeO/Fe2O3ÔºåÁõ¥Êé•ÈáçÂëΩÂêçÂπ∂ËÆ°ÁÆóÊÄª FeO
        df = df.rename(columns={"FeO": "FeOre", "Fe2O3": "Fe2O3re"})
        df["FeO_total"] = df["FeOre"] + df["Fe2O3re"] * 0.8998
    else:
        # Ëã•Âè™Êúâ FeOTÔºåÂàôÊåâÂ∞ñÊô∂Áü≥ÈÖç‰ΩçÂÅáËÆæÊãÜÂàÜ Fe2+/Fe3+
        def fe_split_spinel(row, O_basis=32):
            val_feot = row.get('FeOT', np.nan)
            val_feot = 0.0 if pd.isna(val_feot) else float(val_feot)

            moles = {ox: row[ox]/MW[ox] for ox in MW if ox != 'FeO'}
            moles['FeO'] = val_feot / MW['FeO']

            O_total = sum(moles[ox] * O_num[ox] for ox in moles)
            fac = O_basis / O_total if O_total > 0 else 0.0

            cations = {ox: moles[ox] * Cat_num[ox] * fac for ox in moles}
            S = sum(cations.values())
            T = 24.0

            Fe_total_apfu = cations['FeO']
            Fe3_apfu = max(0.0, 2 * O_basis * (1 - T / S)) if S > 0 else 0.0
            Fe3_apfu = min(Fe3_apfu, Fe_total_apfu)
            Fe2_apfu = Fe_total_apfu - Fe3_apfu

            Fe2_frac = Fe2_apfu / Fe_total_apfu if Fe_total_apfu > 0 else 0.0
            Fe3_frac = Fe3_apfu / Fe_total_apfu if Fe_total_apfu > 0 else 0.0

            FeO_wt   = Fe2_frac * val_feot
            Fe2O3_wt = Fe3_frac * val_feot * FE2O3_OVER_FEO_FE_EQ

            return pd.Series({
                'FeOre': FeO_wt,
                'Fe2O3re': Fe2O3_wt,
                'Fe2_frac': Fe2_frac,
                'Fe3_frac': Fe3_frac,
                'FeO_total': FeO_wt + Fe2O3_wt * 0.8998
            })

        df = df.join(df.apply(fe_split_spinel, axis=1))

    mol_wt = {'Cr2O3':151.99,'Al2O3':101.961,'MgO':40.304,'FeO':71.844,'Fe2O3':159.688}
    Cr_mol = df["Cr2O3"] / mol_wt["Cr2O3"] * 2
    Al_mol = df["Al2O3"] / mol_wt["Al2O3"] * 2
    Mg_mol = df["MgO"] / mol_wt["MgO"]
    Fe2_mol = df["FeOre"] / mol_wt["FeO"]
    Fe3_mol = df["Fe2O3re"] / mol_wt["Fe2O3"] * 2

    # Ê¥æÁîüÁâπÂæÅÔºà‰∏é‰Ω†ÂéüÁâà‰∏ÄËá¥Ôºâ
    df["Cr_CrplusAl"] = Cr_mol / (Cr_mol + Al_mol)
    df["Mg_MgplusFe"] = Mg_mol / (Mg_mol + Fe2_mol)
    df["FeCrAlFe"]   = Fe3_mol / (Fe3_mol + Cr_mol + Al_mol)
    df["FeMgFe"]     = Fe2_mol / (Fe2_mol + Mg_mol)
    return df

def to_numeric_df(df):
    """Â∞ΩÈáèÊääÊâÄÊúâÂàóËΩ¨ floatÔºåÊó†Ê≥ïËΩ¨Êç¢ÂàôÁΩÆ NaN„ÄÇ"""
    return df.apply(pd.to_numeric, errors="coerce")

# ========= ÁªÑÁªìÊûúÔºöÂçïÂ±ÇÂ§öÊï∞Á•® + Âπ≥ÂùáÊ¶ÇÁéá =========
def summarize_level_top_share(labels, maxp):
    """
    labels: ËØ•Â±ÇÊØèË°åÁöÑÈ¢ÑÊµãÔºàÂ¶Ç Level3 ÁöÑ pred3_labelÔºâ
    maxp:   ËØ•Â±ÇÊØèË°å‚ÄúË¢´Âà§ÂÆöÁ±ªÂà´‚ÄùÁöÑÊ¶ÇÁéáÔºàÈòàÂÄº/Á∫¶ÊùüÂêéÁöÑÈÇ£‰∏ÄÂàóÔºõÊú™ÂèÇ‰∏éËØ•Â±ÇÁöÑË°åÂèØ‰∏∫ NaNÔºâ

    ËøîÂõû:
      None  -> Ê≤°ÊúâÊúâÊïàÊ†áÁ≠æÔºàÂÖ® Unknown/NaN/Á©∫Ôºâ
      dict  -> {"label","agree","total","share","prob"}
               prob = ‰ªÖÂú®ËØ•Á±ªÂà´ÁöÑÈÇ£‰∫õË°å‰∏äÁöÑÂπ≥ÂùáÊ¶ÇÁéá
    """
    s = pd.Series(labels, dtype="object")
    m = s.notna() & (s != "") & (s != ABSTAIN_LABEL)
    total = int(m.sum())
    if total == 0:
        return None

    s_valid = s[m]
    p_valid = pd.Series(maxp, dtype="float64")[m]

    counts = s_valid.value_counts()
    max_count = counts.max()
    candidates = counts[counts == max_count].index.tolist()

    means = {c: float(p_valid[s_valid == c].mean()) for c in candidates}
    best = max(candidates, key=lambda c: means[c])

    return {"label": best, "agree": int(max_count), "total": total,
            "share": max_count/total, "prob": means[best]}

# -------------------- Âä†ËΩΩÊ®°Âûã & ÁâπÂæÅÔºà‰æßËæπÊ†èÔºâ --------------------
with st.sidebar:
    st.subheader("Model Loading")
    try:
        model_lvl1, model_lvl2, model_lvl3, feature_list = load_model_and_metadata()
        st.success("Models and feature list loaded.")
        st.caption(f"Feature dimension: {len(feature_list)}")
    except Exception as e:
        st.error("Failed to load models or feature columns.")
        st.exception(e)

# -------------------- ‰∏ä‰º†Êñá‰ª∂Âπ∂Â§ÑÁêÜÔºàUI Ëã±ÊñáÔºâ --------------------
uploaded_file = st.file_uploader("Upload an Excel or CSV file (must include all feature columns).", type=["xlsx", "csv"])

if uploaded_file is not None:
    try:
        if uploaded_file.name.lower().endswith(".csv"):
            df_uploaded = pd.read_csv(uploaded_file)
        else:
            df_uploaded = pd.read_excel(uploaded_file)

        # È¢ÑÂ§ÑÁêÜÔºàÂê´ Fe ÊãÜÂàÜ & Ê¥æÁîüÁâπÂæÅÔºâ
        df_uploaded = preprocess_uploaded_data(df_uploaded)

        # ÂØπÈΩêÁâπÂæÅÂàóÔºàÁº∫Â§±Ë°• NaNÔºåÊåâËÆ≠ÁªÉÂàóÈ°∫Â∫èÂèñÔºâ
        df_input = df_uploaded.copy()
        for col in feature_list:
            if col not in df_input.columns:
                df_input[col] = np.nan
        df_input = to_numeric_df(df_input[feature_list])

        # -------------------- ‰∏âÁ∫ßÊé®ÁêÜÔºà‰∏éËÆ≠ÁªÉÂè£ÂæÑ‰∏ÄËá¥Ôºâ --------------------
        # Level1Ôºà‰∏çÂêØÁî® UnknownÔºâ
        prob1 = model_lvl1.predict_proba(df_input)
        classes1 = model_lvl1.classes_.astype(str)
        pred1_idx = np.argmax(prob1, axis=1)
        pred1_label = classes1[pred1_idx]
        p1max = prob1[np.arange(len(df_input)), pred1_idx]  # L1 ÊúÄÂ§ßÊ¶ÇÁéá

        # Level2Ôºà‰ªÖ L1=ExtraterrestrialÔºõÈòàÂÄºÊîæË°å -> UnknownÔºâ
        _pred1_norm = (
            pd.Series(pred1_label, dtype="object").astype("string").str.strip().str.lower().fillna("")
        )
        mask_lvl2 = (_pred1_norm == "extraterrestrial").to_numpy()

        prob2 = np.full((len(df_input), len(model_lvl2.classes_)), np.nan)
        pred2_label = np.full(len(df_input), "", dtype=object)
        p2max = np.full(len(df_input), np.nan)

        if mask_lvl2.any():
            prob2_masked = model_lvl2.predict_proba(df_input[mask_lvl2])
            classes2 = model_lvl2.classes_.astype(str)
            pred2_masked, p2max_masked = apply_threshold(prob2_masked, classes2, THRESHOLDS["Level2"])
            prob2[mask_lvl2] = prob2_masked
            pred2_label[mask_lvl2] = pred2_masked
            p2max[mask_lvl2] = p2max_masked

        # Level3ÔºàÁà∂Â≠êÁ∫¶Êùü + ÈòàÂÄºÊîæË°åÔºâ
        _pred2_norm = (
            pd.Series(pred2_label, dtype="object").astype("string").str.strip().str.lower().fillna("")
        )
        mask_lvl3 = _pred2_norm.isin(["oc", "cc"]).to_numpy()

        prob3 = np.full((len(df_input), len(model_lvl3.classes_)), np.nan)
        pred3_label = np.full(len(df_input), "", dtype=object)
        p3max = np.full(len(df_input), np.nan)

        if mask_lvl3.any():
            all_proba3 = model_lvl3.predict_proba(df_input[mask_lvl3])
            classes3 = model_lvl3.classes_.astype(str)
            idxs = np.where(mask_lvl3)[0]
            for row_i, i_global in enumerate(idxs):
                parent = str(pred2_label[i_global])          # "OC" Êàñ "CC"
                allowed = valid_lvl3.get(parent, set())
                p = all_proba3[row_i].copy()
                if allowed:
                    mask_allowed = np.isin(classes3, list(allowed))
                    p = p * mask_allowed
                    if p.sum() > 0:
                        p = p / p.sum()
                j = int(np.argmax(p))
                pmax = float(p[j])
                pred3_label[i_global] = classes3[j] if pmax >= THRESHOLDS["Level3"] else ABSTAIN_LABEL
                p3max[i_global] = pmax
            prob3[mask_lvl3] = all_proba3

        # -------------------- Â±ïÁ§∫ÁªìÊûúÔºàÂàóÂêçËã±ÊñáÔºâ --------------------
        df_display = df_uploaded.copy().reset_index(drop=True)
        df_display.insert(0, "Index", df_display.index + 1)
        df_display.insert(1, "Level1_Pred", pred1_label)
        df_display.insert(2, "Level2_Pred", pred2_label)
        df_display.insert(3, "Level3_Pred", pred3_label)

        for i, c in enumerate(model_lvl1.classes_.astype(str)):
            df_display[f"P_Level1_{c}"] = prob1[:, i]
        for i, c in enumerate(model_lvl2.classes_.astype(str)):
            df_display[f"P_Level2_{c}"] = prob2[:, i]
        for i, c in enumerate(model_lvl3.classes_.astype(str)):
            df_display[f"P_Level3_{c}"] = prob3[:, i]

        st.subheader("üßæ Predictions")
        st.dataframe(df_display)

        # -------------------- SHAP ÂèØËß£ÈáäÊÄß --------------------
        st.subheader("üìà SHAP Interpretability")
        cols = st.columns(3)
        for col, (model, name) in zip(cols, [(model_lvl1, "Level1"), (model_lvl2, "Level2"), (model_lvl3, "Level3")]):
            with col:
                st.markdown(f"#### üîç {name} Model")
                explainer = _make_explainer_cached(_model_signature(model), _model=model)
                shap_values = explainer.shap_values(df_input)
                shap.summary_plot(shap_values, df_input, plot_type="bar", show=False)
                st.pyplot(plt.gcf()); plt.close()
                shap.summary_plot(shap_values, df_input, show=False)
                st.pyplot(plt.gcf()); plt.close()

        # -------------------- ‚úÖ Êñ∞ÊùøÂùóÔºöÊ†∑ÂìÅ‰∏ÄËá¥ÊÄßÁ°ÆËÆ§ + ÁªÑÁªìÊûúÔºà‰∏âÁ∫ßÈÉΩÊØîËæÉÔºâ --------------------
        st.subheader("üß™ Specimen Confirmation & Group Result")
        same_specimen = st.checkbox("I confirm all uploaded rows originate from the same physical specimen.")
        if same_specimen:
            # ÂàÜÂà´ËÆ°ÁÆó Level1/2/3 ÁöÑÂ§öÊï∞Á•®+Âπ≥ÂùáÊ¶ÇÁéá
            sum_L1 = summarize_level_top_share(pred1_label, p1max)
            sum_L2 = summarize_level_top_share(pred2_label, p2max)
            sum_L3 = summarize_level_top_share(pred3_label, p3max)

            cands = []
            if sum_L1: cands.append(("Level1", sum_L1))
            if sum_L2: cands.append(("Level2", sum_L2))
            if sum_L3: cands.append(("Level3", sum_L3))

            if cands:
                # ÂÖàÊØîÂç†ÊØî shareÔºåÂÜçÊØîÂπ≥ÂùáÊ¶ÇÁéá probÔºåÊúÄÂêéÂÅèÂêëÊõ¥ÁªÜÂ±ÇÁ∫ß L3>L2>L1
                depth = {"Level1": 1, "Level2": 2, "Level3": 3}
                final_level, final = sorted(
                    cands,
                    key=lambda t: (t[1]["share"], t[1]["prob"], depth[t[0]]),
                    reverse=True
                )[0]

                st.success(
                    f"Final group result ‚Üí **{final_level}: {final['label']}**  |  "
                    f"Probability (mean for this class): **{final['prob']:.3f}**  |  "
                    f"Share: **{final['agree']}/{final['total']} ({final['share']:.0%})**"
                )

                # Â∞èË°®Ôºö‰∏âÂ±ÇÂØπÊØîÔºåÂøÉÈáåÊõ¥ÊúâÂ∫ï
                rows = []
                for lvl, s in [("Level1", sum_L1), ("Level2", sum_L2), ("Level3", sum_L3)]:
                    if s:
                        rows.append({
                            "Level": lvl,
                            "Top class": s["label"],
                            "Share": f"{s['agree']}/{s['total']} ({s['share']:.0%})",
                            "Mean prob": round(s["prob"], 3),
                        })
                if rows:
                    st.dataframe(pd.DataFrame(rows))
            else:
                st.info("No valid predictions available to summarize this group.")

        # -------------------- ËÆ≠ÁªÉÊ±† & GitHub ÂêåÊ≠• --------------------
        st.subheader("üß© Add Predictions to Training Pool?")
        if st.checkbox("‚úÖ Confirm to append these samples to the training pool for future retraining"):
            df_save = df_input.copy()
            df_save["Level1"] = pred1_label
            df_save["Level2"] = pred2_label
            df_save["Level3"] = pred3_label
            local_path = "training_pool.csv"
            header_needed = not os.path.exists(local_path)
            df_save.to_csv(local_path, mode="a", header=header_needed, index=False, encoding="utf-8-sig")
            st.success("‚úÖ Samples appended to local training pool.")

            try:
                GITHUB_TOKEN = (
                    st.secrets.get("gh_token")
                    or (st.secrets.get("github", {}) or {}).get("token")
                )
                repo_owner = st.secrets.get("gh_repo_owner", "Farah-rain")
                repo_name  = st.secrets.get("gh_repo_name",  "chromite")
                dst_path   = st.secrets.get("gh_dst_path",   "training_pool.csv")
                branch     = st.secrets.get("gh_branch",     "main")

                if not GITHUB_TOKEN:
                    st.info("GitHub token not configured (gh_token or github.token). Saved locally only.")
                else:
                    with open(local_path, "rb") as f:
                        content_b64 = base64.b64encode(f.read()).decode("utf-8")
                    url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{dst_path}"
                    headers = {"Authorization": f"token {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}
                    r = requests.get(url, headers=headers)
                    sha = r.json().get("sha") if r.status_code == 200 else None
                    payload = {"message": "update training pool", "content": content_b64, "branch": branch}
                    if sha: payload["sha"] = sha
                    put_resp = requests.put(url, headers=headers, json=payload)
                    if 200 <= put_resp.status_code < 300:
                        st.success("‚úÖ Synced to GitHub repository.")
                    else:
                        st.warning(f"‚ö†Ô∏è GitHub sync failed ({put_resp.status_code}): {put_resp.text[:300]}")
            except Exception as e:
                st.error(f"‚ùå GitHub sync error: {e}")

        # -------------------- ÁªìÊûú‰∏ãËΩΩ --------------------
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df_display.to_excel(writer, index=False, sheet_name='Prediction')
        st.download_button(
            label="üì• Download Predictions (Excel)",
            data=output.getvalue(),
            file_name="prediction_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    except Exception as e:
        st.error("Error while processing the uploaded file.")
        st.exception(e)
else:
    st.info("Please upload a data file to proceed.")
