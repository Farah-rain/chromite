
import streamlit as st
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import os, joblib, requests, base64
from io import BytesIO

# -------------------- é¡µé¢é…ç½®ï¼ˆä»… UI è‹±æ–‡ï¼‰ --------------------
st.set_page_config(page_title="Chromite Extraterrestrial Origin Classifier", layout="wide")
st.title("âœ¨ Chromite Extraterrestrial Origin Classifier")

# -------------------- å¸¸é‡ä¸æ˜ å°„ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼›æ³¨é‡Šä¸­æ–‡ï¼‰ --------------------
ABSTAIN_LABEL = "Unknown"  # Unknown æ ‡ç­¾ç»Ÿä¸€å£å¾„
THRESHOLDS = {"Level2": 0.90, "Level3": 0.90}  # Level2/Level3 çš„æ”¾è¡Œé˜ˆå€¼
valid_lvl3 = {  # çˆ¶å­çº¦æŸï¼ˆè‹¥æ¨¡å‹æ²¡æœ‰èšåˆç±» "UOC"ï¼Œæ”¹ä¸º UOC-H/L/LLï¼‰
    "OC": {"EOC-H", "EOC-L", "EOC-LL", "UOC"},
    "CC": {"CM-CO", "CR-clan", "CV"}
}

# -------------------- å°å·¥å…·å‡½æ•° --------------------
def apply_threshold(proba: np.ndarray, classes: np.ndarray, thr: float):
    """å¯¹åˆ†ç±»æ¦‚ç‡åº”ç”¨é˜ˆå€¼ï¼šæœ€å¤§æ¦‚ç‡>=thr æ—¶è¾“å‡ºè¯¥ç±»åˆ«ï¼Œå¦åˆ™ Unknownã€‚è¿”å›(é¢„æµ‹, æœ€å¤§æ¦‚ç‡)ã€‚"""
    max_idx = np.argmax(proba, axis=1)
    max_val = proba[np.arange(proba.shape[0]), max_idx]
    pred = np.where(max_val >= thr, classes[max_idx], ABSTAIN_LABEL)
    return pred, max_val

@st.cache_resource
def load_model_and_metadata():
    """æ¨¡å‹ä¸ç‰¹å¾åˆ—åŠ è½½ï¼šä¼˜å…ˆ models/ ç›®å½•ï¼›ç‰¹å¾åˆ—ä¼˜å…ˆ JSON é€€å› model.feature_name_ã€‚"""
    def _load(p1, p2): return joblib.load(p1) if os.path.exists(p1) else joblib.load(p2)
    model_lvl1 = _load("models/model_level1.pkl", "model_level1.pkl")
    model_lvl2 = _load("models/model_level2.pkl", "model_level2.pkl")
    model_lvl3 = _load("models/model_level3.pkl", "model_level3.pkl")

    feat_json = "models/feature_columns.json" if os.path.exists("models/feature_columns.json") else "feature_columns.json"
    if os.path.exists(feat_json):
        import json
        with open(feat_json, "r", encoding="utf-8") as f: features = json.load(f)
    else:
        features = getattr(model_lvl1, "feature_name_", None)
        if not features:
            st.error("Feature columns not found (feature_columns.json or model.feature_name_).")
            st.stop()
    return model_lvl1, model_lvl2, model_lvl3, features

@st.cache_resource
def _make_explainer_cached(sig: str, _model):
    """ç¼“å­˜ SHAP explainerï¼›sig ä½œä¸ºç¼“å­˜é”®ï¼Œ_model ä¸å‚ä¸å“ˆå¸Œã€‚"""
    return shap.TreeExplainer(_model)

def _model_signature(model) -> str:
    """æ„é€ ä¸€ä¸ªå¯å“ˆå¸Œçš„æ¨¡å‹ç­¾åï¼šæ¨¡å‹ç±»å + æ’åºåçš„è¶…å‚ + ç±»åˆ«åˆ—è¡¨"""
    try:    params_tup = tuple(sorted((k, str(v)) for k, v in model.get_params().items()))
    except Exception: params_tup = ()
    try:    classes = tuple(map(str, getattr(model, "classes_", ())))
    except Exception: classes = ()
    return f"{model.__class__.__name__}|{hash(params_tup)}|{hash(classes)}"

def preprocess_uploaded_data(df):
    """æ•°æ®é¢„å¤„ç†ï¼šå…¼å®¹ FeOT ç¼ºå¤±çš„æ‹†åˆ†ï¼›æ´¾ç”Ÿç‰¹å¾ç”Ÿæˆï¼ˆä¸ä½ åŸé€»è¾‘ä¸€è‡´ï¼‰ã€‚"""
    MW = {'TiO2':79.866,'Al2O3':101.961,'Cr2O3':151.99,'FeO':71.844,'MnO':70.937,'MgO':40.304,'ZnO':81.38,'SiO2':60.0843,'V2O3':149.88}
    O_num={'TiO2':2,'Al2O3':3,'Cr2O3':3,'FeO':1,'MnO':1,'MgO':1,'ZnO':1,'SiO2':2,'V2O3':3}
    Cat_num={'TiO2':1,'Al2O3':2,'Cr2O3':2,'FeO':1,'MnO':1,'MgO':1,'ZnO':1,'SiO2':1,'V2O3':2}
    FE2O3_OVER_FEO_FE_EQ = 159.688 / (2 * 71.844)

    for ox in MW:
        if ox not in df.columns: df[ox] = 0.0
    df = df.copy()

    if "FeO" in df.columns and "Fe2O3" in df.columns:
        df = df.rename(columns={"FeO": "FeOre", "Fe2O3": "Fe2O3re"})
        df["FeO_total"] = df["FeOre"] + df["Fe2O3re"] * 0.8998
    else:
        def fe_split_spinel(row, O_basis=32):
            val_feot = 0.0 if pd.isna(row.get('FeOT', np.nan)) else float(row.get('FeOT'))
            moles = {ox: row[ox]/MW[ox] for ox in MW if ox != 'FeO'}
            moles['FeO'] = val_feot / MW['FeO']
            O_total = sum(moles[ox]*O_num[ox] for ox in moles)
            fac = O_basis / O_total if O_total>0 else 0.0
            cations = {ox: moles[ox]*Cat_num[ox]*fac for ox in moles}
            S = sum(cations.values()); T = 24.0
            Fe_total = cations['FeO']
            Fe3 = max(0.0, 2*O_basis*(1 - T/S)) if S>0 else 0.0
            Fe3 = min(Fe3, Fe_total); Fe2 = Fe_total - Fe3
            Fe2_frac = Fe2/Fe_total if Fe_total>0 else 0.0
            Fe3_frac = Fe3/Fe_total if Fe_total>0 else 0.0
            FeO_wt   = Fe2_frac * val_feot
            Fe2O3_wt = Fe3_frac * val_feot * FE2O3_OVER_FEO_FE_EQ
            return pd.Series({'FeOre':FeO_wt,'Fe2O3re':Fe2O3_wt,'Fe2_frac':Fe2_frac,'Fe3_frac':Fe3_frac,'FeO_total':FeO_wt+Fe2O3_wt*0.8998})
        df = df.join(df.apply(fe_split_spinel, axis=1))

    mol_wt = {'Cr2O3':151.99,'Al2O3':101.961,'MgO':40.304,'FeO':71.844,'Fe2O3':159.688}
    Cr_mol = df["Cr2O3"]/mol_wt["Cr2O3"]*2
    Al_mol = df["Al2O3"]/mol_wt["Al2O3"]*2
    Mg_mol = df["MgO"]/mol_wt["MgO"]
    Fe2_mol = df["FeOre"]/mol_wt["FeO"]
    Fe3_mol = df["Fe2O3re"]/mol_wt["Fe2O3"]*2

    df["Cr#"] = Cr_mol / (Cr_mol + Al_mol)
    df["Mg#"] = Mg_mol / (Mg_mol + Fe2_mol)
    df["Fe*"]   = Fe3_mol / (Fe3_mol + Cr_mol + Al_mol)
    df["Fe#"]     = Fe2_mol / (Fe2_mol + Mg_mol)
    return df

def to_numeric_df(df):
    """å°½é‡æŠŠæ‰€æœ‰åˆ—è½¬ floatï¼Œæ— æ³•è½¬æ¢åˆ™ç½® NaNã€‚"""
    return df.apply(pd.to_numeric, errors="coerce")

# ========= å•å±‚å¤šæ•°ç¥¨ + å¹³å‡æ¦‚ç‡ï¼ˆå« Unknown & æœªè·¯ç”±è¡Œï¼‰=========
def level_group_stats(labels, classes, prob_by_class, p_max=None, p_unknown=None, fill_unknown_for_empty=True):
    """
    labels: è¯¥å±‚æ¯è¡Œçš„æ ‡ç­¾ï¼ˆå¯å«ç©ºä¸²/Unknownï¼‰
    classes: è¯¥å±‚ç±»åˆ«æ•°ç»„ï¼ˆé¡ºåºä¸ prob_by_class åˆ—ä¸€è‡´ï¼‰
    prob_by_class: (N, C) è¯¥å±‚â€œæœ€ç»ˆç”¨äºåˆ¤å®šâ€çš„æ¯ç±»æ¦‚ç‡ï¼ˆL3 ç”¨çˆ¶å­çº¦æŸåçš„ pï¼‰
    p_max: æ¯è¡Œè¯¥å±‚çš„æœ€å¤§æ¦‚ç‡ï¼ˆçº¦æŸåï¼‰ï¼Œç”¨äºè®¡ç®— Unknown æ¦‚ç‡
    p_unknown: è‹¥ä¼  None åˆ™ç”¨ (1 - p_max)
    fill_unknown_for_empty: True æ—¶ï¼Œæœªè·¯ç”±ï¼ˆç©ºä¸²ï¼‰è§†ä¸º Unknownï¼ŒUnknown æ¦‚ç‡=1.0ï¼Œå®ç±»æ¦‚ç‡=0
    è¿”å›: (top_label, top_share_str '17/18', top_mean_prob)
    """
    N = len(labels)
    s = pd.Series(labels, dtype="object").fillna("")
    if fill_unknown_for_empty:
        empty_mask = (s == "")
        if empty_mask.any():
            s.loc[empty_mask] = ABSTAIN_LABEL
            if p_unknown is None and p_max is not None:
                p_unknown = np.where(empty_mask, 1.0, (1.0 - (p_max if p_max is not None else 0.0)))
            elif p_unknown is not None:
                p_unknown = np.where(empty_mask, 1.0, p_unknown)
            if prob_by_class is not None and isinstance(prob_by_class, np.ndarray):
                prob_by_class = np.where(
                    np.repeat(empty_mask.values[:, None], prob_by_class.shape[1], axis=1),
                    0.0,
                    np.nan_to_num(prob_by_class, nan=0.0)
                )

    counts = s.value_counts()
    candidates = list(counts.index)

    means = {}
    for lab in candidates:
        if lab == ABSTAIN_LABEL:
            if p_unknown is None and p_max is not None:
                pu = 1.0 - np.array(p_max, dtype=float)
            else:
                pu = np.array(p_unknown, dtype=float) if p_unknown is not None else np.zeros(N)
            means[lab] = float(np.nanmean(pu))
        else:
            if prob_by_class is None:
                means[lab] = 0.0
            else:
                col = np.where(classes == lab)[0]
                if len(col) == 0:
                    means[lab] = 0.0
                else:
                    arr = np.nan_to_num(prob_by_class[:, col[0]], nan=0.0)
                    means[lab] = float(np.mean(arr))

    max_count = counts.max()
    top_cands = [lab for lab, c in counts.items() if c == max_count]
    top_label = max(top_cands, key=lambda lab: means.get(lab, 0.0))
    top_share_str = f"{int(counts[top_label])}/{N}"
    top_mean_prob = means.get(top_label, 0.0)
    return top_label, top_share_str, top_mean_prob

# -------------------- ä¾§è¾¹æ ï¼šåŠ è½½æ¨¡å‹ --------------------
with st.sidebar:
    st.subheader("Model Loading")
    try:
        model_lvl1, model_lvl2, model_lvl3, feature_list = load_model_and_metadata()
        st.success("Models and feature list loaded.")
        st.caption(f"Feature dimension: {len(feature_list)}")
    except Exception as e:
        st.error("Failed to load models or feature columns.")
        st.exception(e)

# -------------------- ä¸Šä¼ æ–‡ä»¶å¹¶å¤„ç† --------------------
uploaded_file = st.file_uploader("Upload an Excel or CSV file (must include all feature columns).", type=["xlsx", "csv"])

if uploaded_file is not None:
    try:
        df_uploaded = pd.read_csv(uploaded_file) if uploaded_file.name.lower().endswith(".csv") else pd.read_excel(uploaded_file)
        df_uploaded = preprocess_uploaded_data(df_uploaded)

        # å¯¹é½ç‰¹å¾åˆ—
        df_input = df_uploaded.copy()
        for col in feature_list:
            if col not in df_input.columns: df_input[col] = np.nan
        df_input = to_numeric_df(df_input[feature_list])

        N = len(df_input)

        # ========= Level 1ï¼ˆä¸å¯ç”¨ Unknownï¼‰=========
        prob1 = model_lvl1.predict_proba(df_input)            # (N, C1)
        classes1 = model_lvl1.classes_.astype(str)
        pred1_idx = np.argmax(prob1, axis=1)
        pred1_label = classes1[pred1_idx]
        p1max = prob1[np.arange(N), pred1_idx]                # ä»…å±•ç¤ºï¼›L1 æ—  Unknown

        # ========= Level 2ï¼ˆé˜ˆå€¼ + Unknownï¼Œä»… L1=Extraterrestrialï¼‰=========
        _pred1_norm = pd.Series(pred1_label, dtype="object").astype("string").str.strip().str.lower().fillna("")
        mask_lvl2 = (_pred1_norm == "extraterrestrial").to_numpy()

        prob2_raw = np.full((N, len(model_lvl2.classes_)), np.nan)
        pred2_label = np.full(N, "", dtype=object)
        p2max = np.full(N, np.nan)
        p2unk = np.full(N, np.nan)

        if mask_lvl2.any():
            pr2 = model_lvl2.predict_proba(df_input[mask_lvl2])
            classes2 = model_lvl2.classes_.astype(str)
            pred2_masked, p2max_masked = apply_threshold(pr2, classes2, THRESHOLDS["Level2"])
            prob2_raw[mask_lvl2] = pr2
            pred2_label[mask_lvl2] = pred2_masked
            p2max[mask_lvl2] = p2max_masked
            p2unk[mask_lvl2] = 1.0 - p2max_masked

        # å¯¹æœªè·¯ç”±è¡Œï¼šè§†ä¸º Unknownï¼›Unknown æ¦‚ç‡=1ï¼›å®ç±»æ¦‚ç‡=0
        classes2 = model_lvl2.classes_.astype(str)
        prob2 = np.nan_to_num(prob2_raw, nan=0.0)
        empty2 = (pd.Series(pred2_label, dtype="object") == "")
        if empty2.any():
            pred2_label[empty2.values] = ABSTAIN_LABEL
            p2unk[empty2.values] = 1.0

        # ========= Level 3ï¼ˆçˆ¶å­çº¦æŸ + é˜ˆå€¼ + Unknownï¼Œä»… L2 in {OC,CC}ï¼‰=========
        _pred2_norm = pd.Series(pred2_label, dtype="object").astype("string").str.strip().str.lower().fillna("")
        mask_lvl3 = _pred2_norm.isin(["oc", "cc"]).to_numpy()
        routed_to_L3 = bool(mask_lvl3.any())   # â˜† å…³é”®ï¼šæ•´ç»„æ˜¯å¦â€œåªåˆ°äºŒçº§â€

        C3 = len(model_lvl3.classes_)
        classes3 = model_lvl3.classes_.astype(str)
        prob3_raw = np.full((N, C3), np.nan)
        prob3_post = np.zeros((N, C3))
        pred3_label = np.full(N, "", dtype=object)
        p3max = np.full(N, np.nan)
        p3unk = np.full(N, np.nan)

        if routed_to_L3:
            all_pr3 = model_lvl3.predict_proba(df_input[mask_lvl3])
            idxs = np.where(mask_lvl3)[0]
            prob3_raw[mask_lvl3] = all_pr3
            for row_i, i_global in enumerate(idxs):
                parent = str(pred2_label[i_global])          # "OC" æˆ– "CC"
                allowed = valid_lvl3.get(parent, set())
                p = all_pr3[row_i].copy()
                if allowed:
                    mask_allowed = np.isin(classes3, list(allowed))
                    p = p * mask_allowed
                    if p.sum() > 0:
                        p = p / p.sum()
                j = int(np.argmax(p)); pmax = float(p[j])
                pred3_label[i_global] = classes3[j] if pmax >= THRESHOLDS["Level3"] else ABSTAIN_LABEL
                p3max[i_global] = pmax
                p3unk[i_global] = 1.0 - pmax
                prob3_post[i_global] = p

            # æœªè·¯ç”±åˆ° L3 çš„è¡Œï¼ˆè¿™æ—¶å­˜åœ¨ï¼‰ï¼šè®¾ä¸º Unknownï¼ŒUnknown æ¦‚ç‡=1
            empty3 = (pd.Series(pred3_label, dtype="object") == "")
            if empty3.any():
                pred3_label[empty3.values] = ABSTAIN_LABEL
                p3unk[empty3.values] = 1.0

        # -------------------- æ„é€ å±•ç¤ºè¡¨ --------------------
        df_display = df_uploaded.copy().reset_index(drop=True)
        df_display.insert(0, "Index", df_display.index + 1)
        df_display.insert(1, "Level1_Pred", pred1_label)
        df_display.insert(2, "Level2_Pred", pred2_label)

        # åŸå§‹å„ç±»æ¦‚ç‡ï¼ˆä¾¿äºæ ¸æŸ¥ï¼‰
        for i, c in enumerate(classes1): df_display[f"P_Level1_{c}"] = prob1[:, i]
        for i, c in enumerate(classes2): df_display[f"P_Level2_{c}"] = prob2[:, i]

        # åªæœ‰å½“æ•´ç»„æœ‰æ ·æœ¬è·¯ç”±åˆ° L3 æ—¶æ‰æ·»åŠ  L3 åˆ—
        if routed_to_L3:
            df_display.insert(3, "Level3_Pred", pred3_label)
            for i, c in enumerate(classes3):
                df_display[f"P_Level3_{c}"] = prob3_raw[:, i]  # åŸå§‹ï¼ˆæœªå¿…çº¦æŸåï¼‰

        st.subheader("ğŸ§¾ Predictions")
        st.dataframe(df_display)

        # -------------------- ç»„å†…å¤šæ•°ç¥¨ + å‡å€¼æ¦‚ç‡ï¼ˆUnknownå‚ä¸ï¼›åˆ†æ¯=Nï¼‰ --------------------
        # L1ï¼ˆæ—  Unknownï¼‰
        l1_label, l1_share, l1_mean = level_group_stats(
            labels=pred1_label, classes=classes1, prob_by_class=prob1,
            p_max=p1max, p_unknown=None, fill_unknown_for_empty=False
        )
        # L2ï¼ˆæœ‰ Unknownï¼‰
        l2_label, l2_share, l2_mean = level_group_stats(
            labels=pred2_label, classes=classes2, prob_by_class=prob2,
            p_max=p2max, p_unknown=p2unk, fill_unknown_for_empty=True
        )
        # L3ï¼ˆåªæœ‰å½“ routed_to_L3 ä¸ºçœŸæ—¶æ‰è®¡ç®—ï¼‰
        if routed_to_L3:
            l3_label, l3_share, l3_mean = level_group_stats(
                labels=pred3_label, classes=classes3, prob_by_class=prob3_post,
                p_max=p3max, p_unknown=p3unk, fill_unknown_for_empty=True
            )

        # æŠŠç»„çº§ç»Ÿè®¡å†™å›è¡¨ï¼ˆæ¯è¡Œç›¸åŒï¼Œä¾¿äºå¯¼å‡º/ç­›é€‰ï¼‰
        df_display["L1_TopShare"]    = l1_share
        df_display["L1_TopMeanProb"] = round(l1_mean, 3)
        df_display["L2_TopShare"]    = l2_share
        df_display["L2_TopMeanProb"] = round(l2_mean, 3)
        if routed_to_L3:
            df_display["L3_TopShare"]    = l3_share
            df_display["L3_TopMeanProb"] = round(l3_mean, 3)


       
# -------------------- ğŸ“ˆ SHAP Interpretability --------------------

# -------------------- ğŸ“ˆ SHAP Interpretability --------------------
        # -------------------- ğŸ“ˆ SHAP Interpretability --------------------
        st.subheader("ğŸ“ˆ SHAP Interpretability")

        def _safe_class_names(m):   # å–çœŸå®ç±»åˆ«åå¹¶è½¬æˆå­—ç¬¦ä¸²
            try:
                return [str(x) for x in list(getattr(m, "classes_", []))]
            except Exception:
                return []

        def _bar_per_class(shap_vals_1class, X, title, top_k=15):
            """
            è‡ªå·±ç”»æ¡å½¢å›¾ï¼šæŒ‰è¯¥â€œç±»åˆ«â€çš„ mean(|SHAP|) å–å‰ top_k ä¸ªç‰¹å¾ã€‚
            è¿™æ ·å®Œå…¨é¿å… shap å†…ç½® legend é‡Œå‡ºç° class1/2/3ã€‚
            """
            mean_abs = np.mean(np.abs(shap_vals_1class), axis=0)
            order = np.argsort(mean_abs)[-top_k:]                # å– Top-K
            feats = X.columns.values[order]
            vals  = mean_abs[order]

            fig, ax = plt.subplots(figsize=(6, 3 + 0.2*len(order)))
            ax.barh(range(len(vals)), vals)
            ax.set_yticks(range(len(vals)))
            ax.set_yticklabels(feats)
            ax.set_xlabel("mean |SHAP|")
            ax.set_title(title)
            fig.tight_layout()
            st.pyplot(fig)
            plt.close(fig)

        def _render_shap_for_model(model, level_name, X):
            """
            å…¼å®¹å¤šåˆ†ç±»(list) / äºŒåˆ†ç±»(ndarray)ï¼š
            - å¤šåˆ†ç±»ï¼šæ¯ä¸ªâ€œçœŸå®ç±»åˆ«åâ€å„ç”»ä¸€å¼ æ¡å½¢å›¾ + å„ç”»ä¸€å¼ èœ‚ç¾¤å›¾ï¼›
            - äºŒåˆ†ç±»ï¼šæŠŠ shap ndarray æ‹†æˆâ€œè´Ÿç±»= -svã€æ­£ç±»= svâ€ï¼ŒåŒæ ·å„ç”»ä¸€å¼ ã€‚
            """
            explainer = _make_explainer_cached(_model_signature(model), _model=model)
            sv = explainer.shap_values(X)
            class_names = _safe_class_names(model)

            # å¤šåˆ†ç±»ï¼šsv æ˜¯ list[n_classes]
            if isinstance(sv, list):
                # é€ç±»ç”»ï¼šæ¡å½¢å›¾ + èœ‚ç¾¤å›¾ï¼ˆæ ‡é¢˜å†™çœŸå®ç±»åˆ«åï¼‰
                for i, cname in enumerate(class_names or [f"class {i}" for i in range(len(sv))]):
                    _bar_per_class(sv[i], X, title=f"{level_name} â€” class: {cname}")
                    shap.summary_plot(sv[i], X, show=False)
                    plt.title(f"{level_name} â€” SHAP beeswarm (class: {cname})")
                    st.pyplot(plt.gcf()); plt.close()

            else:
                # äºŒåˆ†ç±»å¸¸è¿”å›å•ä¸ª ndarrayï¼ˆæ­£ç±»çš„ SHAPï¼‰
                # æˆ‘ä»¬æ‰‹å·¥æ„é€ ä¸¤ä¸ªç±»åˆ«çš„è§†å›¾ï¼šè´Ÿç±» = -svï¼Œæ­£ç±» = sv
                if len(class_names) == 2:
                    sv_list  = [ -sv, sv ]
                    names    = [ class_names[0], class_names[1] ]
                else:
                    # æå°‘æ•°æƒ…å†µä¸‹æ‹¿ä¸åˆ° classes_ï¼›è‡³å°‘ä¿è¯æ­£ç±»èƒ½ç”»
                    sv_list  = [ sv ]
                    names    = [ class_names[-1] if class_names else "positive" ]

                for arr, cname in zip(sv_list, names):
                    _bar_per_class(arr, X, title=f"{level_name} â€” class: {cname}")
                    shap.summary_plot(arr, X, show=False)
                    plt.title(f"{level_name} â€” SHAP beeswarm (class: {cname})")
                    st.pyplot(plt.gcf()); plt.close()

        cols = st.columns(3)
        for col, (mdl, nm) in zip(cols, [(model_lvl1, "Level1"), (model_lvl2, "Level2"), (model_lvl3, "Level3")]):
            with col:
                st.markdown(f"#### ğŸ” {nm} Model")
                _render_shap_for_model(mdl, nm, df_input)

        
       

        # -------------------- âœ… æ ·å“ä¸€è‡´æ€§ + ç»„ç»“æœï¼ˆæ ¹æ®æ˜¯å¦å­˜åœ¨ L3 åŠ¨æ€å±•ç¤ºï¼‰ --------------------
        st.subheader("ğŸ§ª Specimen Confirmation & Group Result")
        same_specimen = st.checkbox("I confirm all uploaded rows originate from the same physical specimen.")
        if same_specimen:
            depth = {"Level1": 1, "Level2": 2, "Level3": 3}
            cands = [
                ("Level1", {"label": l1_label, "share": int(l1_share.split('/')[0]) / N, "prob": l1_mean,
                            "agree": int(l1_share.split('/')[0]), "total": N}),
                ("Level2", {"label": l2_label, "share": int(l2_share.split('/')[0]) / N, "prob": l2_mean,
                            "agree": int(l2_share.split('/')[0]), "total": N}),
            ]
            if routed_to_L3:
                cands.append(("Level3", {"label": l3_label, "share": int(l3_share.split('/')[0]) / N, "prob": l3_mean,
                                         "agree": int(l3_share.split('/')[0]), "total": N}))
            final_level, final = sorted(cands, key=lambda t: (t[1]["share"], t[1]["prob"], depth[t[0]]), reverse=True)[0]
            st.success(
                f"Final group result â†’ **{final_level}: {final['label']}**  |  "
                f"Probability (mean for this class): **{final['prob']:.3f}**  |  "
                f"Share: **{final['agree']}/{final['total']} ({final['share']:.0%})**"
            )

            # å¯¹æ¯”å°è¡¨
            rows = [
                {"Level": "Level1", "Top class": l1_label, "Share": l1_share, "Mean prob": round(l1_mean, 3)},
                {"Level": "Level2", "Top class": l2_label, "Share": l2_share, "Mean prob": round(l2_mean, 3)},
            ]
            if routed_to_L3:
                rows.append({"Level": "Level3", "Top class": l3_label, "Share": l3_share, "Mean prob": round(l3_mean, 3)})
            st.dataframe(pd.DataFrame(rows))

        # -------------------- ğŸ§© æ¢å¤ï¼šè®­ç»ƒæ±  & GitHub åŒæ­¥ --------------------
        st.subheader("ğŸ§© Add Predictions to Training Pool?")
        if st.checkbox("âœ… Confirm to append these samples to the training pool for future retraining"):
            df_save = df_input.copy()
            df_save["Level1"] = pred1_label
            df_save["Level2"] = pred2_label
            if routed_to_L3:
                df_save["Level3"] = pred3_label
            # ä¹Ÿå¯æŠŠç»„ç»“æœå†™å…¥ï¼ˆå¦‚éœ€å¯è§£æ³¨é‡Šï¼‰
            # df_save["Group_L1_Top"] = l1_label; df_save["Group_L2_Top"] = l2_label
            # if routed_to_L3: df_save["Group_L3_Top"] = l3_label

            local_path = "training_pool.csv"
            header_needed = not os.path.exists(local_path)
            df_save.to_csv(local_path, mode="a", header=header_needed, index=False, encoding="utf-8-sig")
            st.success("âœ… Samples appended to local training pool.")

            try:
                GITHUB_TOKEN = (
                    st.secrets.get("gh_token")
                    or (st.secrets.get("github", {}) or {}).get("token")
                )
                repo_owner = st.secrets.get("gh_repo_owner", "Farah-rain")
                repo_name  = st.secrets.get("gh_repo_name",  "chromite")
                dst_path   = st.secrets.get("gh_dst_path",   "training_pool.csv")
                branch     = st.secrets.get("gh_branch",     "main")

                if not GITHUB_TOKEN:
                    st.info("GitHub token not configured (gh_token or github.token). Saved locally only.")
                else:
                    with open(local_path, "rb") as f:
                        content_b64 = base64.b64encode(f.read()).decode("utf-8")
                    url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{dst_path}"
                    headers = {"Authorization": f"token {GITHUB_TOKEN}", "Accept": "application/vnd.github+json"}
                    r = requests.get(url, headers=headers)
                    sha = r.json().get("sha") if r.status_code == 200 else None
                    payload = {"message": "update training pool", "content": content_b64, "branch": branch}
                    if sha: payload["sha"] = sha
                    put_resp = requests.put(url, headers=headers, json=payload)
                    if 200 <= put_resp.status_code < 300:
                        st.success("âœ… Synced to GitHub repository.")
                    else:
                        st.warning(f"âš ï¸ GitHub sync failed ({put_resp.status_code}): {put_resp.text[:300]}")
            except Exception as e:
                st.error(f"âŒ GitHub sync error: {e}")

        # -------------------- ç»“æœä¸‹è½½ --------------------
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df_display.to_excel(writer, index=False, sheet_name='Prediction')
        st.download_button(
            label="ğŸ“¥ Download Predictions (Excel)",
            data=output.getvalue(),
            file_name="prediction_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    except Exception as e:
        st.error("Error while processing the uploaded file.")
        st.exception(e)
else:
    st.info("Please upload a data file to proceed.")
